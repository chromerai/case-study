{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CODE FOR SCRAPING THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "def scrape_product_page(url):\n",
    "    print(f\"Scraping product page: {url}\")\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    description = soup.find('div', class_='pd__description pd__wrap mt-3').find('div').text.strip() if soup.find('div', class_='pd__description pd__wrap mt-3') else ''\n",
    "    #price = soup.find('span', class_='price pd__price')['content'] if soup.find('span', class_='price') else ''\n",
    "    #video_link = soup.find('div', class_='yt-video').find('iframe')['src'] if soup.find('div', class_='yt-video') else ''\n",
    "    symptoms_header = soup.find('div', class_='pd__wrap row').find('div', class_='bold mb-1')\n",
    "    symptoms = symptoms_header.find_next_sibling(text=True).strip().split('|')\n",
    "    symptoms = [s.strip() for s in symptoms]\n",
    "    qna = soup.find_all('div', class_='qna__question js-qnaResponse')\n",
    "    qna_list = []\n",
    "\n",
    "    for q in qna:\n",
    "        q = q.find_all('div', class_='js-searchKeys')\n",
    "        qna_list.append({\n",
    "            'question': q[0].text.strip(),\n",
    "            'answer': q[1].text.strip()})\n",
    "    \n",
    "    find_models = soup.find('div', class_='pd__crossref__list js-dataContainer js-infiniteScroll')\n",
    "    model_list_container = find_models.find_all('div', class_='row')\n",
    "    model_list = []\n",
    "    for e in model_list_container:\n",
    "        model_list.append({'Brand': e.find('div', class_='col-6 col-md-3').text.strip(), 'Model': e.find('a').text.strip(), 'model_link': e.find('a')['href']})\n",
    "    \n",
    "    return {\n",
    "        'description': description,\n",
    "        #'assist_video': video_link,\n",
    "        'compatible_models_list': model_list,\n",
    "        'qna_list': qna_list,\n",
    "        'Symptoms_fixed': symptoms \n",
    "    }\n",
    "\n",
    "def scrape_category_parts(category_url):\n",
    "    base_url = \"https://www.partselect.com\"\n",
    "    all_parts = []\n",
    "    \n",
    "    response = requests.get(category_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the list of all part categories\n",
    "    all_nf_links = soup.find_all('ul', class_='nf__links')\n",
    "    if len(all_nf_links) >= 2:\n",
    "        part_categories = all_nf_links[1]\n",
    "        if part_categories:\n",
    "            for li in part_categories.find_all('li'):\n",
    "                subcategory_url = base_url + li.find('a')['href']\n",
    "                subcategory_name = li.find('a').text.strip()\n",
    "            \n",
    "                print(f\"Scraping subcategory: {subcategory_name}\")\n",
    "            \n",
    "            # Scrape each subcategory\n",
    "                subcategory_response = requests.get(subcategory_url)\n",
    "                subcategory_soup = BeautifulSoup(subcategory_response.content, 'html.parser')\n",
    "            \n",
    "                parts = subcategory_soup.find_all('div', class_='nf__part__detail')\n",
    "            \n",
    "                for part in parts[:3]:\n",
    "                    a = part.find('a', class_='nf__part__detail__title')\n",
    "                    name = a.find('span').text.strip()\n",
    "                    ps_number = part.find('div', class_='nf__part__detail__part-number').find('strong').text.strip()\n",
    "                    mfg_number = part.find('div', class_='nf__part__detail__part-number mb-2').find('strong').text.strip()\n",
    "                \n",
    "                    product_url = base_url + a['href']\n",
    "                \n",
    "                    product_info = scrape_product_page(product_url)\n",
    "                \n",
    "                    all_parts.append({\n",
    "                        'name': name,\n",
    "                        'ps_number': ps_number,\n",
    "                        'mfg_number': mfg_number,\n",
    "                        'category': subcategory_name,\n",
    "                        'product_url': product_url,\n",
    "                        'description': product_info['description'],\n",
    "                        'qna_list': product_info['qna_list'],\n",
    "                        'compatible_models': product_info['compatible_models_list'],\n",
    "                        'Symptoms fixed': product_info['Symptoms_fixed']\n",
    "                    })\n",
    "                \n",
    "                    time.sleep(1)  # Be nice to the server\n",
    "    \n",
    "    return all_parts\n",
    "\n",
    "def scrape_partselect():\n",
    "    base_url = \"https://www.partselect.com\"\n",
    "    categories = [\n",
    "        \"/Refrigerator-Parts.htm\",\n",
    "        \"/Dishwasher-Parts.htm\"\n",
    "    ]\n",
    "    \n",
    "    all_parts = []\n",
    "\n",
    "    for category in categories:\n",
    "        url = base_url + category\n",
    "        print(f\"Scraping category: {category}\")\n",
    "        category_parts = scrape_category_parts(url)\n",
    "        all_parts.extend(category_parts)\n",
    "    \n",
    "    return all_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data = scrape_partselect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data[0]['compatible_models']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON\n",
    "with open('partselect_data.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(scraped_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Scraped {len(scraped_data)} parts and saved to partselect_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***ADDING DATA TO THE GRAPH DATABASE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from create_database import Neo4jLoader\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv\n",
    "\n",
    "# Set up OpenAI API key\n",
    "\n",
    "loader = Neo4jLoader(os.environ['NEO4J_URI'], os.environ['NEO4J_USER'], os.environ['NEO4J_PASSWORD'])\n",
    "\n",
    "# Load your parts data\n",
    "with open('partselect_data.json', 'r') as f:\n",
    "    parts_data = json.load(f)\n",
    "\n",
    "loader.load_data(parts_data)\n",
    "\n",
    "loader.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Dict, Any, List, TypedDict, Annotated\n",
    "# from langgraph.graph import StateGraph, START, END \n",
    "# from neo4j import GraphDatabase\n",
    "# from openai import OpenAI\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from IPython.display import Image, display\n",
    "# from dotenv import load_dotenv\n",
    "# import json\n",
    "\n",
    "\n",
    "\n",
    "# # Initialize Neo4j connection and OpenAI client\n",
    "# NEO4J_URI = ''\n",
    "# NEO4J_USER = \"\"\n",
    "# NEO4J_PASSWORD = \"\"\n",
    "# driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "# load_dotenv()\n",
    "# client = OpenAI()\n",
    "\n",
    "\n",
    "# class State(TypedDict):\n",
    "#     user_input: str\n",
    "#     conversation_history: Annotated[List[str], \"mutable\"]\n",
    "#     extracted_info: Dict[str, str]\n",
    "#     tool_output: Dict[str, Any]\n",
    "#     next_step: str\n",
    "#     tool_explanation: Annotated[List[str], \"mutable\"]\n",
    "#     generated_response: str\n",
    "#     feedback: str\n",
    "    \n",
    "# # Helper functions\n",
    "# def query_neo4j(query, params=None):\n",
    "#     with driver.session() as session:\n",
    "#         result = session.run(query, params)\n",
    "#         return [record.data() for record in result]\n",
    "\n",
    "# def embed_text(text: str) -> List[float]:\n",
    "#     embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#     response = embedder.encode([text])\n",
    "#     return response.data[0].embedding\n",
    "\n",
    "# class RecommendationTool:\n",
    "#     def __init__(self, uri: str, user: str, password: str, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "#         self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "#         self.model = SentenceTransformer(model_name)\n",
    "\n",
    "#     def recommend(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "#         # Convert query to embedding\n",
    "#         embedding = self.model.encode(query).tolist()\n",
    "\n",
    "#         with self.driver.session() as session:\n",
    "#             return session.read_transaction(self._similarity_search, embedding, top_k)\n",
    "\n",
    "#     def _similarity_search(self, tx, embedding: List[float], top_k: int) -> List[Dict]:\n",
    "#         query = \"\"\"\n",
    "#         CALL db.index.vector.queryNodes('part_embeddings', $top_k, $embedding) \n",
    "#         YIELD node, score\n",
    "#         RETURN node.name AS name, node.description AS description, score\n",
    "#         \"\"\"\n",
    "#         result = tx.run(query, top_k=top_k, embedding=embedding)\n",
    "#         return [dict(record) for record in result]\n",
    "\n",
    "#     def close(self):\n",
    "#         self.driver.close()\n",
    "\n",
    "# # Define tools\n",
    "# class InfoRetrievalTool:\n",
    "#     def __init__(self, uri, user, password):\n",
    "#         self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "#     def __call__(self, extracted_info: Dict[str, str]) -> Dict[str, Any]:\n",
    "#         with self.driver.session() as session:\n",
    "#             return session.read_transaction(self._get_part_info, extracted_info)\n",
    "\n",
    "#     def _get_part_info(self, tx, extracted_info: Dict[str, str]) -> Dict[str, Any]:\n",
    "#         query = \"\"\"\n",
    "#         MATCH (p:Part)\n",
    "#     WHERE \n",
    "#     ($ps_number <> '' AND p.ps_number = $ps_number) OR\n",
    "#     ($mfg_number <> '' AND p.mfg_number = $mfg_number)\n",
    "\n",
    "# OPTIONAL MATCH (p)-[:COMPATIBLE_WITH]->(m:Model)\n",
    "# WHERE $model_number = '' OR m.number = $model_number\n",
    "\n",
    "# OPTIONAL MATCH (p)-[:FIXES]->(s:Symptom)\n",
    "# WHERE $symptom = '' OR s.name = $symptom\n",
    "\n",
    "# WITH p, \n",
    "#      CASE WHEN $model_number <> '' AND $symptom <> '' \n",
    "#           THEN collect(DISTINCT m) ELSE [] END as models,\n",
    "#      CASE WHEN $model_number <> '' AND $symptom <> '' \n",
    "#           THEN collect(DISTINCT s) ELSE [] END as symptoms\n",
    "\n",
    "# RETURN {\n",
    "#     name: p.name,\n",
    "#     description: p.description,\n",
    "#     ps_number: p.ps_number,\n",
    "#     mfg_number: p.mfg_number,\n",
    "#     models: CASE WHEN $model_number <> '' AND $symptom <> '' \n",
    "#                  THEN [m IN models | m.number] ELSE [] END,\n",
    "#     symptoms_fixed: CASE WHEN $model_number <> '' AND $symptom <> '' \n",
    "#                          THEN [s IN symptoms | s.name] ELSE [] END,\n",
    "#     part_url: p.url\n",
    "# } as part_info\n",
    "#         \"\"\"\n",
    "        \n",
    "#         result = tx.run(query, \n",
    "#                         ps_number=extracted_info.get('ps_number', ''),\n",
    "#                         model_number=extracted_info.get('model', ''),\n",
    "#                         mfg_number=extracted_info.get('mfg_number', ''),\n",
    "#                         symptom=extracted_info.get('symptom', ''))\n",
    "        \n",
    "#         records = list(result)\n",
    "#         print(records)\n",
    "#         if not records:\n",
    "#             return {\"message\": \"No matching part found.\"}\n",
    "        \n",
    "#         return records[0]['part_info']\n",
    "\n",
    "#     def close(self):\n",
    "#         self.driver.close()\n",
    "\n",
    "# class SymptomAnalysisTool:\n",
    "#     def __init__(self, neo4j_uri: str, neo4j_user: str, neo4j_password: str):\n",
    "#         self.driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "#         self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#     def __call__(self, model: str, symptom: str) -> Dict[str, Any]:\n",
    "#         parts = self._find_relevant_parts(model, symptom)\n",
    "#         relevant_qas = self._find_relevant_qas(parts, symptom)\n",
    "#         return {\n",
    "#             \"model\": model,\n",
    "#             \"symptom\": symptom,\n",
    "#             \"relevant_parts\": parts,\n",
    "#             \"relevant_qas\": relevant_qas\n",
    "#         }\n",
    "\n",
    "#     def _find_relevant_parts(self, model: str, symptom: str) -> List[Dict[str, Any]]:\n",
    "#         print(\" in find relevant parts\")\n",
    "#         with self.driver.session() as session:\n",
    "#             result = session.run(\"\"\"\n",
    "#                 MATCH (m:Model {name: $model})-[:HAS_ISSUE]->(s:Symptom {name: $symptom})<-[:FIXES]-(p:Part)\n",
    "#                 RETURN p.name AS name, p.ps_number AS ps_number, p.mfg_number AS mfg_number,\n",
    "#                        p.description AS description\n",
    "#                 LIMIT 2\n",
    "#             \"\"\", model=model, symptom=symptom)\n",
    "#             print(\"relevant parts\", result)\n",
    "#             return [dict(record) for record in result]\n",
    "\n",
    "#     def _find_relevant_qas(self, parts: List[Dict[str, Any]], symptom: str) -> List[Dict[str, Any]]:\n",
    "#         combined_embedding = self.model.encode(symptom + \" \" + \" \".join([p['name'] + p['ps_number'] for p in parts])).tolist()\n",
    "        \n",
    "#         with self.driver.session() as session:\n",
    "#             result = session.run(\"\"\"\n",
    "#             CALL db.index.vector.queryNodes('question_embedding', 10, $embedding) \n",
    "#             YIELD node as question, score\n",
    "#             MATCH (question)-[:HAS_ANSWER]->(answer:Answer)\n",
    "#             RETURN question.text AS question, answer.text AS answer, score\n",
    "#             ORDER BY score DESC\n",
    "#             LIMIT 5\n",
    "#         \"\"\", embedding=combined_embedding)\n",
    "            \n",
    "#             print(\"relevant qas\", result)\n",
    "#             return [dict(record) for record in result]\n",
    "\n",
    "#     def close(self):\n",
    "#         self.driver.close()\n",
    "\n",
    "\n",
    "# class CompatibilityCheckerTool:\n",
    "#     def __call__(self, part: str, model: str) -> Dict[str, Any]:\n",
    "#         query = \"\"\"\n",
    "#         MATCH (p:Part)\n",
    "# WHERE p.ps_number = $part OR p.mfg_number = $part\n",
    "# MATCH (p)-[:COMPATIBLE_WITH]->(m:Model {name: $model})\n",
    "# RETURN COUNT(*) > 0 AS is_compatible\n",
    "#         \"\"\"\n",
    "#         result = query_neo4j(query, {\"part\": part, \"model\": model})\n",
    "#         return {\"is_compatible\": result[0][\"is_compatible\"] if result else False}\n",
    "\n",
    "# # Create tool instances\n",
    "# info_retrieval_tool = InfoRetrievalTool(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "# symptom_analysis_tool = SymptomAnalysisTool(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "# compatibility_checker_tool = CompatibilityCheckerTool()\n",
    "# recommendation_tool = RecommendationTool(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "\n",
    "# def User_input(state) -> Dict[str, Any]:\n",
    "#     user_input = input(\"User: \")\n",
    "#     state[\"user_input\"] = user_input\n",
    "#     state[\"conversation_history\"].append(f\"User: {user_input}\")\n",
    "#     state[\"next_step\"] = \"central_agent\"\n",
    "#     if user_input == \"exit\":\n",
    "#         state[\"next_step\"] = \"end\"\n",
    "#     return state\n",
    "\n",
    "# # Update tool_manager to use these tools\n",
    "# def tool_manager(state) -> Dict[str, Any]:\n",
    "#     conversation = ' '.join(state[\"conversation_history\"][-5:])\n",
    "#     extracted_info = state[\"extracted_info\"]\n",
    "#     prompt = f\"\"\"\n",
    "#     You are the tools manager for agentic architecture. Based on the following conversation, determine which tool(s) to use:\n",
    "#     {conversation}\n",
    "\n",
    "#     Available tools:\n",
    "#     1. Info Retrieval Tool: Use GraphRAG to get information about parts, models, and their relationships.\n",
    "#     2. Symptom Analysis Tool: Find the relevant parts that can fix the symptom provided the model. Also finds relevant Q&A to figure out the problem and solution.\n",
    "#     3. Compatibility Checker Tool: Check compatibility between parts and models.\n",
    "#     4. Part recommendations Tool: Recommend parts based on the user's query.\n",
    "\n",
    "#     Think step by step. \n",
    "#     Select the most appropriate tool(s). Format your response as follows:\n",
    "#     Selected Tools: [1, 2, 3]\n",
    "#     Explanation: [Your explanation]\n",
    "#     \"\"\"\n",
    "\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4\",\n",
    "#         messages=[{\"role\": \"system\", \"content\": prompt}]\n",
    "#     )\n",
    "\n",
    "#     tool_selection = response.choices[0].message.content\n",
    "#     for line in tool_selection.split(\"\\n\"):\n",
    "#         if \"Selected Tools:\" in line:\n",
    "#             selected_tools = line.split(\":\")[1].strip().strip(\"[]\").split(\",\")\n",
    "#         if \"Explanation:\" in line:\n",
    "#             state[\"tool_explanation\"] = line.split(\":\")[1].strip()\n",
    "\n",
    "#     # Parse tool selection and use appropriate tools\n",
    "#     tool_outputs = {}\n",
    "\n",
    "#     if '1' in selected_tools:\n",
    "#         tool_outputs[\"info_retrieval\"] = info_retrieval_tool(extracted_info)\n",
    "#     if '2' in selected_tools:\n",
    "#         model = extracted_info[\"model\"]\n",
    "#         symptom = extracted_info[\"symptom\"]\n",
    "#         if not model or not symptom:\n",
    "#             return (\"Model and symptom are required for symptom analysis\")\n",
    "#         tool_outputs[\"symptom_analysis\"] = symptom_analysis_tool(symptom, model)\n",
    "\n",
    "#     if '3' in selected_tools:\n",
    "#         part = extracted_info[\"ps_number\"]\n",
    "#         if not part:\n",
    "#             part = extracted_info[\"mfg_number\"]\n",
    "#         model = extracted_info[\"model\"]\n",
    "#         if not part or not model:\n",
    "#             return (\"Part and model are required for compatibility check\")\n",
    "#         tool_outputs[\"compatibility_check\"] = compatibility_checker_tool(part, model)\n",
    "\n",
    "#     if '4' in selected_tools:\n",
    "#         user_query = state[\"user_input\"]\n",
    "#         tool_outputs[\"part_recommendations\"] = recommendation_tool.recommend(user_query)\n",
    "#         tool_outputs[\"part_recommendations\"] = \"Recommendations based on user query\"\n",
    "\n",
    "#     state[\"tool_output\"] = tool_outputs\n",
    "#     state[\"next_step\"] = \"response_generation\"\n",
    "#     return state\n",
    "\n",
    "# def central_agent(state) -> Dict[str, Any]:\n",
    "#     if \"conversation_history\" not in state:\n",
    "#         state[\"conversation_history\"] = []\n",
    "\n",
    "#     user_input = state[\"user_input\"]\n",
    "#     state[\"conversation_history\"].append(f\"User: {user_input}\")\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "#     You are a helpful assistant for a refrigerator and dishwasher parts e-commerce website.\n",
    "#     Your task is to gather all necessary information from the user to assist them effectively.\n",
    "#     If the user hasn't provided enough information, ask follow-up questions. For example, you can ask: \n",
    "#     \"What is the model number of your appliance?\" or mfg number of the part you are looking for?\"\n",
    "\n",
    "#     However, it is also possible that user might be looking for only some general information about a part. So, based on your judgement decide if you need more information or if you can proceed to use tools.\n",
    "\n",
    "#     Sometimes, user will just ask for some part recommendations! by providing a use case or just simply give you a product information. In that case, you can directly proceed to tools.\n",
    "#     Below is the conversation history to give you context:\n",
    "\n",
    "#     Conversation history:\n",
    "#     {' '.join(state[\"conversation_history\"][-5:])}\n",
    "\n",
    "#     Thinks step by step and decide if you need more information or if you can proceed to use tools.\n",
    "#     If you need more information, ask a question. If you have enough information, say \"PROCEED TO TOOLS\".\n",
    "    \n",
    "#     Finally, after you have all the information, extract the following details based on the query asked by the user and additional information provided:\n",
    "#     - mfg number of the part ( if applicable )\n",
    "#     - Part Select number ( PS number ) of the part ( if applicable )\n",
    "#     - model number of the appliance( if applicable )\n",
    "#     - symptom or issue with the appliance ( if applicable)\n",
    "\n",
    "#     Provide your response in a VALID JSON format with the following structure:\n",
    "#     {{\n",
    "#         \"response\": \"Your response text\",\n",
    "#         \"proceed_to_tools\": true/false,\n",
    "#         \"extracted_info\": {{\n",
    "#             \"model\": \"extracted model number or null\",\n",
    "#             \"ps_number\": \"extracted part number or null. This will always start with 'PS' followed by a number\",\n",
    "#             \"mfg_number\": \"extracted manufacturer number or null\",\n",
    "#             \"symptom\": \"extracted symptom/issue or null\"\n",
    "#         }}\n",
    "#     }}\n",
    "#     Ensure that your response is in VALID JSON format. Do not output any markdown or enclose the JSON in triple backticks.\n",
    "#     Your response:\n",
    "#     \"\"\"\n",
    "\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4\",\n",
    "#         messages=[{\"role\": \"system\", \"content\": prompt}]\n",
    "#     )\n",
    "\n",
    "#     ai_response = response.choices[0].message.content\n",
    "#     parsed_response = json.loads(ai_response)\n",
    "#     state[\"conversation_history\"].append(f\"Assistant: {parsed_response['response']}\")\n",
    "#     print(\"central_agent\", parsed_response)\n",
    "\n",
    "#     if parsed_response[\"proceed_to_tools\"]:\n",
    "#         state[\"next_step\"] = \"tool_manager\"\n",
    "#         state['extracted_info'] = parsed_response[\"extracted_info\"]\n",
    "#     else:\n",
    "#         state[\"next_step\"] = \"central_agent\"\n",
    "#         print(\"AI:\", parsed_response['response'])\n",
    "#         state[\"conversation_history\"].append(parsed_response['response'] ) # This will be shown to the user in the actual implementation\n",
    "\n",
    "#     return state\n",
    "\n",
    "# def response_generation(state) -> Dict[str, Any]:\n",
    "#     conversation = ' '.join(state[\"conversation_history\"][-5:])\n",
    "#     tool_output = state[\"tool_output\"]\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "#     Based on the following conversation and tool output, generate a helpful response for the user:\n",
    "\n",
    "#     Conversation:\n",
    "#     {conversation}\n",
    "\n",
    "#     Tool Output:\n",
    "#     {tool_output}\n",
    "\n",
    "#     Provide a detailed and helpful response addressing the user's query:\n",
    "#     \"\"\"\n",
    "\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4\",\n",
    "#         messages=[{\"role\": \"system\", \"content\": prompt}]\n",
    "#     )\n",
    "\n",
    "#     state[\"generated_response\"] = response.choices[0].message.content\n",
    "#     state[\"next_step\"] = \"judge_agent\"\n",
    "#     return state\n",
    "\n",
    "# def judge_agent(state) -> Dict[str, Any]:\n",
    "#     conversation = ' '.join(state[\"conversation_history\"])\n",
    "#     generated_response = state[\"generated_response\"]\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "#     Review the following conversation and generated response:\n",
    "\n",
    "#     Conversation:\n",
    "#     {conversation}\n",
    "\n",
    "#     Generated Response:\n",
    "#     {generated_response}\n",
    "\n",
    "#     Evaluate if the response adequately addresses the user's query. If it does, respond with \"APPROVED\". \n",
    "#     If not, explain what needs to be improved or clarified.\n",
    "\n",
    "#     Your evaluation:\n",
    "#     \"\"\"\n",
    "\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-4\",\n",
    "#         messages=[{\"role\": \"system\", \"content\": prompt}]\n",
    "#     )\n",
    "\n",
    "#     evaluation = response.choices[0].message.content\n",
    "\n",
    "#     if \"APPROVED\" in evaluation:\n",
    "#         state[\"final_response\"] = generated_response\n",
    "#         state[\"next_step\"] = \"end\"\n",
    "#     else:\n",
    "#         state[\"feedback\"] = evaluation\n",
    "#         state[\"next_step\"] = \"central_agent\"\n",
    "#         state[\"user_input\"] = f\"Please improve the response. Feedback: {evaluation}\"\n",
    "\n",
    "#     return state\n",
    "\n",
    "# workflow = StateGraph(State)\n",
    "\n",
    "# # Add nodes\n",
    "# # workflow.add_node(\"User_input\", User_input)\n",
    "# # workflow.add_node(\"Central_agent\",central_agent)\n",
    "# # workflow.add_node(\"Tool_manager\", tool_manager)\n",
    "# # workflow.add_node(\"Response_generation\",response_generation)\n",
    "# # workflow.add_node(\"Judge_agent\",judge_agent)\n",
    "\n",
    "# # # Connect nodes\n",
    "# # workflow.add_edge(START, \"User_input\")\n",
    "# # workflow.add_edge(\"Tool_manager\", \"Response_generation\")\n",
    "# # workflow.add_edge(\"Response_generation\", \"Judge_agent\")\n",
    "\n",
    "# # # Define the conditional logic for agent selection\n",
    "# # def decide_next(state):\n",
    "# #     return state[\"next_step\"]\n",
    "\n",
    "# # # Set the conditional edges\n",
    "# # workflow.add_conditional_edges(\"User_input\", decide_next, {\"central_agent\": \"Central_agent\", \"end\": END} )\n",
    "# # workflow.add_conditional_edges(\"Central_agent\", decide_next, {\"user_input\": \"User_input\", \"tool_manager\": \"Tool_manager\"})\n",
    "# # workflow.add_conditional_edges(\"Judge_agent\", decide_next, {\"central_agent\": \"Central_agent\", \"end\": END})\n",
    "\n",
    "# workflow.add_node(\"Central_agent\", central_agent)\n",
    "# workflow.add_node(\"Tool_manager\", tool_manager)\n",
    "# workflow.add_node(\"Response_generation\", response_generation)\n",
    "# workflow.add_node(\"Judge_agent\", judge_agent)\n",
    "\n",
    "#     # Connect nodes\n",
    "# workflow.add_edge(START, \"Central_agent\")\n",
    "# workflow.add_edge(\"Tool_manager\", \"Response_generation\")\n",
    "# workflow.add_edge(\"Response_generation\", \"Judge_agent\")\n",
    "\n",
    "#     # Define the conditional logic for agent selection\n",
    "# def decide_next(state):\n",
    "#     return state[\"next_step\"]\n",
    "\n",
    "#     # Set the conditional edges\n",
    "# workflow.add_conditional_edges(\"Central_agent\", decide_next, {\"central_agent\": \"Central_agent\", \"tool_manager\": \"Tool_manager\"})\n",
    "# workflow.add_conditional_edges(\"Judge_agent\", decide_next, {\"central_agent\": \"Central_agent\", \"end\": END})\n",
    "\n",
    "# # Compile the graph\n",
    "# app = workflow.compile()\n",
    "# display(Image(app.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# def run_conversation():\n",
    "#     state = State(\n",
    "#         user_input=\"tell me information about part PS429854\",\n",
    "#         conversation_history=[],\n",
    "#         extracted_info={},\n",
    "#         tool_output={},\n",
    "#         next_step=\"User_input\",\n",
    "#         tool_explanation=[],\n",
    "#         generated_response=\"\",\n",
    "#         feedback=\"\"\n",
    "#     )\n",
    "\n",
    "    \n",
    "#         # Invoke the workflow\n",
    "#     result = app.invoke(state)\n",
    "\n",
    "#     # Update the state for the next iteration\n",
    "#     state = result\n",
    "\n",
    "#     # Print the response (assuming it's stored in generated_response)\n",
    "#     print(\"AI:\", state['generated_response'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
